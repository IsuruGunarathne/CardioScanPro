{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Cardio Scan\n"
     ]
    }
   ],
   "source": [
    "print(\"This is Cardio Scan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold,KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_heafile(file_name):\n",
    "    # Open the .hea file\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Read the content of the .hea file\n",
    "        hea_content = file.readlines()\n",
    "\n",
    "    return hea_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(hea_content):\n",
    "    ID = hea_content[0].strip().split()[0]\n",
    "    \n",
    "    # Extract 'Age' from .hea file content\n",
    "    age_info = hea_content[13].strip().split()\n",
    "    age = int(age_info[2]) if len(age_info) > 2 and age_info[2].isdigit() else 0\n",
    "    \n",
    "    # Extract 'Gender' from .hea file content\n",
    "    gender = hea_content[14].strip().split()[2] if len(hea_content) > 14 else 'Unknown'\n",
    "    \n",
    "    # Extract 'Abnormality' from .hea file content\n",
    "    abnormality = hea_content[15].strip().split()[2] if len(hea_content) > 15 else 'Unknown'\n",
    "    \n",
    "    return [ID, age, gender, abnormality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes(training_directory):\n",
    "    dataframes = {}\n",
    "\n",
    "    subdirectories = [subdir for subdir in pl.Path(training_directory).iterdir() if subdir.is_dir()]\n",
    "    \n",
    "    for source_folder_path in subdirectories:\n",
    "        source_folder_name = source_folder_path.name\n",
    "        columns = ['ID', 'Age', 'Gender', 'Abnormality']\n",
    "        source_dataframe = pd.DataFrame(columns=columns)\n",
    "        patient_data = {}  # To collect patient information\n",
    "        \n",
    "        for subdir in source_folder_path.iterdir():\n",
    "            if subdir.is_dir():\n",
    "                data_dir = pl.Path(subdir)\n",
    "                header_files = list(data_dir.glob('*.hea'))\n",
    "\n",
    "                for header_file in header_files:\n",
    "                    header_path = data_dir.joinpath(header_file.name)\n",
    "                    hea_content = read_heafile(header_path)\n",
    "                    patient_info = create_array(hea_content)\n",
    "                    patient_id = patient_info[0]\n",
    "                    \n",
    "                    # Collect patient information\n",
    "                    for i, column_name in enumerate(['Age', 'Gender', 'Abnormality']):\n",
    "                        patient_data.setdefault(patient_id, {})[column_name] = patient_info[i + 1]\n",
    "                        \n",
    "        # Create a list of patient data dictionaries\n",
    "        patient_rows = []\n",
    "        for patient_id, info in patient_data.items():\n",
    "            row = {'ID': patient_id, 'Age': info.get('Age'), 'Gender': info.get('Gender'), 'Abnormality': info.get('Abnormality')}\n",
    "            patient_rows.append(row)\n",
    "            \n",
    "        # Concatenate patient data into the dataframe\n",
    "        source_dataframe = pd.concat([source_dataframe, pd.DataFrame(patient_rows)])\n",
    "        \n",
    "        dataframes[f'{source_folder_name}_df'] = source_dataframe\n",
    "        \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframes('training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dx_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anomalies_array(data):\n",
    "    \"\"\"\n",
    "    This function will take a .csv file as the input.\n",
    "    It will create a array containing all the anomalies\n",
    "    \"\"\"\n",
    "    anomalies_array = []\n",
    "    \n",
    "    for index,row in data.iterrows():\n",
    "        anomalies_array.append(row['SNOMED CT Code'])\n",
    "    \n",
    "    return anomalies_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = create_anomalies_array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_output_array(array,anomalies):\n",
    "    \"\"\"\n",
    "    This will take the anomalies array and the array of anomalies of a patient\n",
    "    This will output an array conatinimg binary values.\n",
    "    It represents the 1 when a patient has the relavent anomaly , otherwise 0\n",
    "    \"\"\"\n",
    "   \n",
    "    data = create_anomalies_array(anomalies)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if(data[i] in array):\n",
    "            data[i] = 1\n",
    "        else:\n",
    "            data[i] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_array(df,anomalies):\n",
    "    \"\"\"\n",
    "    This will take anomalies array and a data frame as the input\n",
    "    This will output the Y data set \n",
    "    \"\"\"\n",
    "    Y = []\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        # Create the anomalies array for the relavent row\n",
    "        # --------code here---------\n",
    "        \n",
    "        array = list(map(int,row['Abnormality'].split(\",\")))\n",
    "        output = create_single_output_array(array,anomalies)\n",
    "        \n",
    "        Y.append(output)\n",
    "        \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srce_files = ['cpsc_2018_df', 'cpsc_2018_extra_df', 'georgia_df', 'ptb_df', 'ptb-xl_df', 'st_petersburg_incart_df']\n",
    "\n",
    "Y = []\n",
    "\n",
    "for ele in srce_files:\n",
    "    y = create_output_array(df[ele],data)\n",
    "    Y = Y + y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for normializing the wave \n",
    "#parameters \n",
    "#  wave form representing the array\n",
    "#  frequency for normalization\n",
    "#  frequency of the waveform\n",
    "def normalize_wave(array,nrm_freq,freq):\n",
    "    factor = round(freq/nrm_freq)\n",
    "    normalized_array = []\n",
    "    for ele in array:\n",
    "        new_ele = ele[::factor]\n",
    "        normalized_array.append(new_ele)\n",
    "    return len(normalized_array[0]),normalized_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mats(dir_path):\n",
    "    # This function will iterate thorugh a data directory and return a list of \n",
    "    # nomlized waveforms for the ECG's in that directory\n",
    "    normalized_waves = []\n",
    "    lengths = []\n",
    "    # Iterating through the subdirectories inside the given directory\n",
    "    for subdir in pl.Path(dir_path).iterdir():\n",
    "        \n",
    "        if subdir.is_dir():\n",
    "            \n",
    "            data_dir = pl.Path(subdir)\n",
    "            \n",
    "            head_file_list = list(data_dir.glob('*.hea'))\n",
    "            mat_file_list = list(data_dir.glob('*.mat'))\n",
    "            for i in range(len(head_file_list)):\n",
    "                head_file_path = data_dir.joinpath(head_file_list[i].name)\n",
    "                mat_file_path = data_dir.joinpath(mat_file_list[i].name)\n",
    "\n",
    "                data = scipy.io.loadmat(mat_file_path)['val']\n",
    "                current_frequency = int(read_heafile(head_file_path)[0].split()[2])\n",
    "                length,nomralized_wave = normalize_wave(data,250,current_frequency)\n",
    "                normalized_waves.append(nomralized_wave)\n",
    "                lengths.append(length)\n",
    "    return lengths,normalized_waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srce_files = ['cpsc_2018', 'cpsc_2018_extra', 'georgia', 'ptb', 'ptb-xl', 'st_petersburg_incart']\n",
    "X = []\n",
    "lengths = []\n",
    "for ele in srce_files:\n",
    "    length,array = normalize_mats('training/' + ele)\n",
    "    lengths = lengths + length\n",
    "    X = X + array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy = X\n",
    "y_copy = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(lengths[:(len(lengths)-74)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [(1000,5000),(5000,10000),(10000,15000),(15000,20000),(20000,25000),(25000,30000),(30000,35000),(35000,40000)]\n",
    "\n",
    "# Create a dictionary to store values for each range\n",
    "range_values = {r: [] for r in ranges}\n",
    "\n",
    "# Categorize values into ranges\n",
    "for value in lengths:\n",
    "    for r in ranges:\n",
    "        if r[0] <= value < r[1]:\n",
    "            range_values[r].append(value)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_labels = [f\"{r[0]}-{r[1]}\" for r in range_values.keys()]\n",
    "lengths_values = [len(values) for values in range_values.values()]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "# Create the bar chart\n",
    "plt.bar(range_labels, lengths_values)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Ranges')\n",
    "plt.ylabel('No of Data points')\n",
    "plt.title('No of data points for Ranges')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lengths_values)\n",
    "print(sum(lengths_values[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sizes = []\n",
    "for i in range(len(lengths)):\n",
    "    if(lengths[i] < 1000 or lengths[i] > 5000):\n",
    "        y_copy[i] = 0\n",
    "        x_copy[i] = 0\n",
    "    else:\n",
    "        new_sizes.append(lengths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy = [item for item in x_copy if type(item) != int]\n",
    "y_copy = [item for item in y_copy if type(item) != int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max :',max(new_sizes))\n",
    "print('min :',min(new_sizes))\n",
    "print('average :',round(stat.mean(new_sizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy_new = []\n",
    "for ele in x_copy:\n",
    "    \n",
    "    size = len(ele[0])\n",
    "    \n",
    "    if(size < 2617):\n",
    "        \n",
    "        start = round((2617 - size)/2)\n",
    "        end = 2617 - size - start\n",
    "        \n",
    "        new_array = []\n",
    "        \n",
    "        for data in ele:\n",
    "\n",
    "            lower_bound,upper_bound = min(data),max(data)\n",
    "            \n",
    "            start_list = [random.randint(lower_bound, upper_bound) for _ in range(start)]\n",
    "            end_list = [random.randint(lower_bound, upper_bound) for _ in range(end)]\n",
    "            \n",
    "            new_sub_array = np.array(start_list + list(data) + end_list)\n",
    "            new_array.append(new_sub_array)\n",
    "   \n",
    "        x_copy_new.append(new_array)\n",
    "    else:\n",
    "        extra = size - 2617\n",
    "        half_extra = round(extra)\n",
    "        \n",
    "        new_array = []\n",
    "        \n",
    "        for data in ele:\n",
    "            new_sub_array = list(data)[(half_extra-1):(half_extra + 2616)]\n",
    "            new_array.append(new_sub_array)\n",
    "        x_copy_new.append(new_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_copy_new)):\n",
    "    x_copy_new[i] = np.array(x_copy_new[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_np_arrays = [ele.shape for ele in x_copy_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_copy)):\n",
    "    y_copy[i] = np.array(y_copy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(x_copy_new), np.array(y_copy), test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])  # Shape: (sequence_length, num_leads)\n",
    "num_classes = y_train.shape[1]  # Number of anomaly classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    identity = x\n",
    "    \n",
    "    x = Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    \n",
    "    if stride != 1:\n",
    "        identity = Conv1D(filters, 1, strides=stride)(identity)\n",
    "    \n",
    "    x = tf.keras.layers.add([x, identity])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=input_shape)\n",
    "x = Conv1D(64, 7, strides=2, padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# Add residual blocks\n",
    "x = residual_block(x, 64, stride=1)\n",
    "x = residual_block(x, 64, stride=1)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "outputs = Dense(num_classes, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Accuracy : \",test_accuracy)\n",
    "print(\"Test Loss : \",test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CardioScanPro_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This array will print the probabilities for each anomaly\n",
    "def get_best_(array):\n",
    "    sorted_array = sorted(array)[::-1]\n",
    "    for ele in sorted_array:\n",
    "        print(str(array.index(ele)) + \" ==> \" + str(ele) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
